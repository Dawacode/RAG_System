import os
import torch
import time
from app.utils.config import MODEL_PATH , BASE_MODEL
from torch.cuda.amp import autocast

from unsloth import FastLanguageModel
from transformers import AutoTokenizer
from app.utils.logger import get_logger

logger = get_logger(__name__, subsystem="app")

model = None
tokenizer = None
model_loaded = False
compute_dtype = torch.float16 


try:
    if not MODEL_PATH or not os.path.isdir(MODEL_PATH):
        raise ValueError(f"MODEL_PATH '{MODEL_PATH}' is not configured correctly or is not a valid directory.")

    logger.info(f"Starting model loading process directly from checkpoint directory...")
    logger.info(f"Checkpoint Path: {MODEL_PATH}")
    logger.info(f"Loading model with 4-bit quantization...")

    requested_dtype = torch.bfloat16
    if not torch.cuda.is_bf16_supported():
        logger.warning("Device does not support bfloat16. Requesting float16 instead for model loading.")
        requested_dtype = torch.float16
        compute_dtype = torch.float16 
    else:
        compute_dtype = torch.bfloat16 
        logger.info("Device supports bfloat16. Requesting bfloat16 for model loading.")


    
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = MODEL_PATH,     
        max_seq_length = 4096,      
        dtype = requested_dtype,     
        load_in_4bit = True,
       
    )

    actual_model_dtype = next(model.parameters()).dtype 
    logger.info(f"Model loaded successfully from '{MODEL_PATH}'.")
    logger.info(f"Requested dtype: {requested_dtype}, Actual model parameter dtype: {actual_model_dtype}")
    logger.info(f"Will use compute dtype: {compute_dtype} for autocast.")

    logger.info("Setting model to evaluation mode.")
    model.eval()

    if hasattr(tokenizer, 'chat_template') and not tokenizer.chat_template:
         logger.warning("Tokenizer does not seem to have a chat template defined.")
    elif not hasattr(tokenizer, 'chat_template'):
         logger.warning("Could not check for tokenizer.chat_template attribute.")


    model_loaded = True
    logger.info("Generator model and tokenizer loaded and configured successfully.")

except Exception as e:
    logger.exception(f"CRITICAL: Failed to load the generator model from CHECKPOINT_PATH='{MODEL_PATH}'. Error: {e}")


def call_gemma3(prompt: str) -> tuple[str, float, int, int]: 
    """
    Generates a response using the loaded Gemma3 model based on a prompt string.
    The prompt is expected to be already formatted with the chat template.

    Args:
        prompt: The complete prompt string to send to the model, including turn tokens.

    Returns:
        A tuple containing:
        - str: The generated response text (raw output from LLM).
        - float: Time taken for generation in seconds.
        - int: Number of tokens in the input prompt.
        - int: Number of new tokens generated by the model.
        Returns ("Error...", 0.0, 0, 0) if generation fails.
    """
    global model, tokenizer, model_loaded, compute_dtype
    if not model_loaded or model is None or tokenizer is None:
        logger.error("Generator model or tokenizer not loaded. Cannot generate response.")
        return "Fel: Modellen för att generera svar kunde inte laddas korrekt.", 0.0, 0, 0 

    logger.info("Received request to generate response from prompt string.")
    logger.debug(f"Prompt length: {len(prompt)} chars.")
    logger.debug(f"Prompt (start): {prompt[:200]}...")

    input_token_count = 0
    generated_token_count = 0
    generation_time = 0.0
    decoded_response = "" 

    try:
        logger.debug("Tokenizing prompt string...")
        tokenizer_start_time = time.time()
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=model.max_seq_length  
        )
        tokenizer_end_time = time.time()

        input_token_count = inputs['input_ids'].shape[-1]
        logger.debug(f"Input token count: {input_token_count}")

        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        logger.debug(f"Input tensor shape: {inputs['input_ids'].shape}, Device: {inputs['input_ids'].device}")

        if inputs['input_ids'].shape[-1] >= model.max_seq_length:
            logger.warning(f"Input length ({inputs['input_ids'].shape[-1]}) is close to or exceeds model max length ({model.max_seq_length}). Response might be truncated or incomplete.")

        logger.debug(f"Generating response from model using autocast with {compute_dtype}...")
        generation_start_time = time.time()
        outputs = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=1024,    
            do_sample=True,          
            temperature=0.5,          
            top_p=0.7,                
            top_k=40,                
            repetition_penalty=1.1,  
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id
        )
        generation_end_time = time.time()
        generation_time = generation_end_time - generation_start_time
        logger.info(f"Model generation complete in {generation_time:.4f} seconds. Output tensor shape: {outputs.shape}")

     
        if outputs.shape[-1] > input_token_count:
            generated_ids = outputs[0][input_token_count:]
            logger.debug(f"Decoding {len(generated_ids)} generated tokens...")
            decoded_response = tokenizer.decode(generated_ids, skip_special_tokens=True)
            generated_token_count = len(generated_ids) 
        else:
             logger.warning("Model generated no new tokens.")
             decoded_response = "" 
             generated_token_count = 0

        logger.debug(f"Decoded response length: {len(decoded_response)} chars.")
        logger.debug(f"Raw Response (start): {decoded_response[:250]}...")

        
        return decoded_response.strip(), generation_time, input_token_count, generated_token_count
        
    except Exception as e:
       
        logger.error(f"Exception caught during generation in call_gemma3: {type(e).__name__}: {e}")
        logger.exception("Full traceback from call_gemma3 generation failure:")

       
        error_capture_time = time.time()
        generation_time_on_error = error_capture_time - generation_start_time if 'generation_start_time' in locals() else 0.0
       
        error_message = "Ett internt fel inträffade när svaret skulle genereras från modellen."
        return error_message, generation_time_on_error, input_token_count, generated_token_count 
        
